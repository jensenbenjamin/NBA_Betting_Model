{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Betting NBA Over Unders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to import the pandas package to do all of our data frame manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next, we will use the pandas package to import the data that we're basing our machine learning model off of using the read_csv function. After that we can use the .head() function to print out the first few rows of our data frame to ensure everything has been imported just the way we like it.  \n",
    "\n",
    "I personally collected this data from the official NBA website under the team stats section as well as box scores section. I will include a link below.  I have a CSV file with over 200 games' worth of data sampled from the 2017-2021 seasons. I did not go any further back than 2017 due to a fundemental shift in how teams play basketball. Due to the quality work of sports data analysts, teams began to acknowledge the value of the 3 point shot and embraced it heavily. Teams like the Houston Rockets and Golden State Warriors pioneered this change. In order to avoid bias introduced by this paradigm shift I omitted years preceeding 2017 altogether.\n",
    "\n",
    "https://www.nba.com/stats/teams/traditional/?sort=W_PCT&dir=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:\\\\NBA_Betting\\\\ModelTrainingandTestingData.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next up, I want to print a list of columns so that I can copy and paste the ones I want into another list to be my features or explanatory/independent variables. With the number of attributes in this data frame the df.columns function does not work because it summarizes and not all column names are displayed in the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TEAM', 'Opponent', 'MATCH\\xa0UP', 'Home Team', 'Away Team', 'GAME\\xa0DATE', 'Game_Id', 'Oppenent_Id', 'Is_Home_Team', 'HomeTeam', 'AwayTeam', 'W/L', 'Home Team Win', 'Point Total', 'MIN', 'PTS', '+/-', 'H_WIN%', 'H_PTS_PG', 'H_FGM_PG', 'H_FGA_PG', 'H_CUM_FG%', 'H_3PM_PG', 'H_3PA_PG', 'H_CUM_3P%', 'H_FTM_PG', 'H_FTA_PG', 'H_CUM_FT%', 'H_OREB_PG', 'H_DREB_PG', 'H_REB_PG', 'H_AST_PG', 'H_TOV_PG', 'H_STL_PG', 'H_BLK_PG', 'H_BLKA_PG', 'H_PF_PG', 'H_PFD_PG', 'H_+/-_PG', 'A_WIN%', 'A_PTS_PG', 'A_FGM_PG', 'A_FGA_PG', 'A_CUM_FG%', 'A_3PM_PG', 'A_3PA_PG', 'A_CUM_3P%', 'A_FTM_PG', 'A_FTA_PG', 'A_CUM_FT%', 'A_OREB_PG', 'A_DREB_PG', 'A_REB_PG', 'A_AST_PG', 'A_TOV_PG', 'A_STL_PG', 'A_BLK_PG', 'H_BLKA_PG2', 'A_PF_PG', 'A_PFD_PG', 'A_+/-_PG', 'HLW_WIN%2', 'HLW_PTS_PG3', 'HLW_FGM_PG4', 'HLW_FGA_PG5', 'HLW_CUM_FG%6', 'HLW_3PM_PG7', 'HLW_3PA_PG8', 'HLW_CUM_3P%9', 'HLW_FTM_PG10', 'HLW_FTA_PG11', 'HLW_CUM_FT%12', 'HLW_OREB_PG13', 'HLW_DREB_PG14', 'HLW_REB_PG15', 'HLW_AST_PG16', 'HLW_TOV_PG17', 'HLW_STL_PG18', 'HLW_BLK_PG19', 'HLW_BLKA_PG20', 'HLW_PF_PG21', 'HLW_PFD_PG22', 'HLW_+/-_PG23', 'ALW_WIN%24', 'ALW_PTS_PG25', 'ALW_FGM_PG26', 'ALW_FGA_PG27', 'ALW_CUM_FG%28', 'ALW_3PM_PG29', 'ALW_3PA_PG30', 'ALW_CUM_3P%31', 'ALW_FTM_PG32', 'ALW_FTA_PG33', 'ALW_CUM_FT%34', 'ALW_OREB_PG35', 'ALW_DREB_PG36', 'ALW_REB_PG37', 'ALW_AST_PG38', 'ALW_TOV_PG39', 'ALW_STL_PG40', 'ALWBLK_PG41', 'HLW_BLKA_PG242', 'ALW_PF_PG43', 'ALW_PFD_PG44', 'ALW_+/-_PG45']\n"
     ]
    }
   ],
   "source": [
    "print(list(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Feature Selection\n",
    "\n",
    "In this step I copy the attributes above that I think most influence in how many points will be scored in a given NBA game. Here are the definitions of the features I chose for both the home and away teams in a game:\n",
    "\n",
    "-Season winning %\n",
    "\n",
    "-Mean points per game scored\n",
    "\n",
    "-Mean fieldgoals scored per game\n",
    "\n",
    "-Mean fieldgoals attempted per game\n",
    "\n",
    "-Season field goal %\n",
    "\n",
    "-^^ Same as the above 3 features for both 3 point shots and free throws\n",
    "\n",
    "-Offensive rebounds per game\n",
    "\n",
    "-defensive rebounds per game\n",
    "\n",
    "-total rebounds per game\n",
    "\n",
    "-assists per game\n",
    "\n",
    "-offensive rebounds per game\n",
    "\n",
    "-defensive rebounds per game\n",
    "\n",
    "-rebounds per game\n",
    "\n",
    "-turnovers per game\n",
    "\n",
    "-steals per game\n",
    "\n",
    "-blocks per game\n",
    "\n",
    "-blocked field goal attempts per game\n",
    "\n",
    "-personal fouls per game\n",
    "\n",
    "-personal fouls drawn per game\n",
    "\n",
    "-Season long average point differential per game\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['H_WIN%', 'H_PTS_PG', 'H_FGM_PG', 'H_FGA_PG', 'H_CUM_FG%', 'H_3PM_PG', 'H_3PA_PG', 'H_CUM_3P%', 'H_FTM_PG', 'H_FTA_PG', 'H_CUM_FT%', 'H_OREB_PG', 'H_DREB_PG', 'H_REB_PG', 'H_AST_PG', 'H_TOV_PG', 'H_STL_PG', 'H_BLK_PG', 'H_BLKA_PG', 'H_PF_PG', 'H_PFD_PG', 'H_+/-_PG', 'A_WIN%', 'A_PTS_PG', 'A_FGM_PG', 'A_FGA_PG', 'A_CUM_FG%', 'A_3PM_PG', 'A_3PA_PG', 'A_CUM_3P%', 'A_FTM_PG', 'A_FTA_PG', 'A_CUM_FT%', 'A_OREB_PG', 'A_DREB_PG', 'A_REB_PG', 'A_AST_PG', 'A_TOV_PG', 'A_STL_PG', 'A_BLK_PG', 'H_BLKA_PG2', 'A_PF_PG', 'A_PFD_PG', 'A_+/-_PG']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean it up!\n",
    "\n",
    "Next up we need to drop all of the rows that have missing values so that our algorithm can run smoothly.  I chose to drop the rows with missing values, but if you would prefer you could do some sort of imputation such as mean or median value imputation. I decided to just drop the rows because there were so few."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_without_missing_values = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the Target\n",
    "\n",
    "In this model we are trying to predict how many points will be scored by the two teams combined.  Earlier, while creating the dataset that I imported as a CSV I had already created this field which was simply a sum of the home team points scored and the away team points scored. Now, we just designate this column as our target or dependent variable. In the next cell I print the first 5 rows of \"target\" to make sure it is what I want it to be.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df_without_missing_values['Point Total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    232\n",
       "1    227\n",
       "2    227\n",
       "3    261\n",
       "4    250\n",
       "Name: Point Total, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X's and y's\n",
    "\n",
    "Next, I just define a data frame with my desired features as X and and a data frame with my target variable as y to fit in with standard conventions for many ML models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_without_missing_values[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data for training and testing\n",
    "\n",
    "We need to split our data into training and testing sets.  This randomly seperates our data into two groups of different sizes (that we choose). The reason is that we don't want to test our model on data it has already seen before. This is called data leakage and it can cause what is called overfitting.  In essence, we are \"hiding\" a third of our data from the model as we train it so that when we want to test it we have games to it to predict that we know the results of, but it doesn't.  Basically, we don't have to wait for the next month's worth of NBA games to happen to find out if our model is any good or not. \n",
    "\n",
    "The reason we have to use data that the model hasn't seen before is because that's what the model is going use in real life. If we test it on data that it has already \"seen\" it will perform better on that test because it learned from those records. The results of that test won't give us any idea of how the model will perform in real life.\n",
    "\n",
    "To perform this split we import the train_test_split function from python's sklearn library. Then we perform the splitting. I chose to make the a third of the data test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages for Model Evaluation\n",
    "\n",
    "This is a regression model.  We need to evaluate it as such. The metric I chose is root mean square error (RMSE). RMSE is the standard deviation of the difference between the predicted values and the actual values. What we want is a low RMSE because this indicates that our data points are close to the \"line of best fit\". I want to note that our line of best fit in this case will not actually be linear because we are using a gradient boosting regressor and not a linear one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It's Time to Run the Model!\n",
    "\n",
    "First up we need to import our gradient boosting model from the sklearn library.  If you want to learn more about how this works and how to use it I encourage you to explore the documentation at:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html\n",
    "\n",
    "We fit the model (let it look at the training data and learn from it) and then we make the predictions for what the scores of those games will be. Then we use the trained model to make a predictions on the test set. Then we evaluate the training and testing predictions using RMSE and print out the results. Remember, the lower the better.  In this case we see that the model performs really well on the training set and not as well on the test set.  This makes sense because the model learned from the training set and has never seen the test set before.  In an ideal case, the numbers would be low and similar, but these results are tolerable.  A large discrepancy between high training performance and low testing performance is an indicator of variance, or overfitting.  The opposite would be called underfitting or bias, where the model doesn't learn enough from the training data and is too simple.\n",
    "\n",
    "Some causes for overfitting are having too many attributes, noisy data, or too much complexity in the model. In this case, I feel like the overfitting comes from redundant attributes and high variance nature of NBA games. Some causes for underfitting are too little data, features that don't explain the target,  or the model is too simple.  We avoid most of that in this case.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (training) for GB:  0.415978\n",
      "RMSE (Test Data) for GB:  7.895790\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "GB = GradientBoostingRegressor()\n",
    "GB.fit(X_train, y_train)\n",
    "GB_predict_Train=GB.predict(X_train)\n",
    "\n",
    "RMSE1=sqrt(mean_squared_error(y_train,GB_predict_Train))\n",
    "print(\"RMSE (training) for GB:{0:10f}\".format(RMSE1))\n",
    "GB_predict_Test=GB.predict(X_test)\n",
    "RMSE= sqrt(mean_squared_error(y_test,GB_predict_Test))\n",
    "print(\"RMSE (Test Data) for GB:{0:10f}\".format(RMSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the Bets\n",
    "\n",
    "Now that we have a model that is ready to predict the final scores of NBA games we can use it to bet the Over/Under. To do this we need to create a data frame with the days NBA schedule. We need the  \"feature\" values for the home and away teams of each game. To do this, I create a CSV with the most up-to-date values available on the NBA site. Then I load that CSV into this file once again using the pandas read_csv function. I also call the .head() function so that I can double check I imported what I wanted to and to show why I am about to drop the first two columns of this data frame.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HomeTeam</th>\n",
       "      <th>AwayTeam</th>\n",
       "      <th>H_WIN%</th>\n",
       "      <th>H_PTS_PG</th>\n",
       "      <th>H_FGM_PG</th>\n",
       "      <th>H_FGA_PG</th>\n",
       "      <th>H_CUM_FG%</th>\n",
       "      <th>H_3PM_PG</th>\n",
       "      <th>H_3PA_PG</th>\n",
       "      <th>H_CUM_3P%</th>\n",
       "      <th>...</th>\n",
       "      <th>A_DREB_PG</th>\n",
       "      <th>A_REB_PG</th>\n",
       "      <th>A_AST_PG</th>\n",
       "      <th>A_TOV_PG</th>\n",
       "      <th>A_STL_PG</th>\n",
       "      <th>A_BLK_PG</th>\n",
       "      <th>H_BLKA_PG2</th>\n",
       "      <th>A_PF_PG</th>\n",
       "      <th>A_PFD_PG</th>\n",
       "      <th>H_+/-_PG23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DEN</td>\n",
       "      <td>CHA</td>\n",
       "      <td>0.587</td>\n",
       "      <td>111.8</td>\n",
       "      <td>41.3</td>\n",
       "      <td>86.2</td>\n",
       "      <td>47.9</td>\n",
       "      <td>12.8</td>\n",
       "      <td>36.1</td>\n",
       "      <td>35.4</td>\n",
       "      <td>...</td>\n",
       "      <td>33.9</td>\n",
       "      <td>44.9</td>\n",
       "      <td>27.5</td>\n",
       "      <td>13.1</td>\n",
       "      <td>8.6</td>\n",
       "      <td>4.9</td>\n",
       "      <td>4.7</td>\n",
       "      <td>19.9</td>\n",
       "      <td>19.8</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ORL</td>\n",
       "      <td>CLE</td>\n",
       "      <td>0.267</td>\n",
       "      <td>104.2</td>\n",
       "      <td>38.2</td>\n",
       "      <td>88.2</td>\n",
       "      <td>43.3</td>\n",
       "      <td>12.0</td>\n",
       "      <td>36.4</td>\n",
       "      <td>33.0</td>\n",
       "      <td>...</td>\n",
       "      <td>34.1</td>\n",
       "      <td>44.4</td>\n",
       "      <td>25.2</td>\n",
       "      <td>14.6</td>\n",
       "      <td>7.1</td>\n",
       "      <td>4.2</td>\n",
       "      <td>4.6</td>\n",
       "      <td>17.2</td>\n",
       "      <td>20.1</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ATL</td>\n",
       "      <td>IND</td>\n",
       "      <td>0.500</td>\n",
       "      <td>112.9</td>\n",
       "      <td>41.2</td>\n",
       "      <td>88.2</td>\n",
       "      <td>46.8</td>\n",
       "      <td>12.6</td>\n",
       "      <td>34.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>...</td>\n",
       "      <td>33.3</td>\n",
       "      <td>44.5</td>\n",
       "      <td>25.0</td>\n",
       "      <td>14.5</td>\n",
       "      <td>6.9</td>\n",
       "      <td>5.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>19.3</td>\n",
       "      <td>-3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BOS</td>\n",
       "      <td>TOR</td>\n",
       "      <td>0.627</td>\n",
       "      <td>110.7</td>\n",
       "      <td>40.3</td>\n",
       "      <td>87.2</td>\n",
       "      <td>46.3</td>\n",
       "      <td>12.9</td>\n",
       "      <td>36.7</td>\n",
       "      <td>35.2</td>\n",
       "      <td>...</td>\n",
       "      <td>31.9</td>\n",
       "      <td>45.3</td>\n",
       "      <td>22.1</td>\n",
       "      <td>12.5</td>\n",
       "      <td>8.9</td>\n",
       "      <td>4.6</td>\n",
       "      <td>5.1</td>\n",
       "      <td>19.7</td>\n",
       "      <td>19.2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHI</td>\n",
       "      <td>NYK</td>\n",
       "      <td>0.581</td>\n",
       "      <td>111.6</td>\n",
       "      <td>41.8</td>\n",
       "      <td>86.9</td>\n",
       "      <td>48.1</td>\n",
       "      <td>10.8</td>\n",
       "      <td>29.2</td>\n",
       "      <td>37.1</td>\n",
       "      <td>...</td>\n",
       "      <td>34.7</td>\n",
       "      <td>46.3</td>\n",
       "      <td>21.6</td>\n",
       "      <td>13.4</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>4.7</td>\n",
       "      <td>20.5</td>\n",
       "      <td>20.4</td>\n",
       "      <td>-0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  HomeTeam AwayTeam  H_WIN%  H_PTS_PG  H_FGM_PG  H_FGA_PG  H_CUM_FG%  \\\n",
       "0      DEN      CHA   0.587     111.8      41.3      86.2       47.9   \n",
       "1      ORL      CLE   0.267     104.2      38.2      88.2       43.3   \n",
       "2      ATL      IND   0.500     112.9      41.2      88.2       46.8   \n",
       "3      BOS      TOR   0.627     110.7      40.3      87.2       46.3   \n",
       "4      CHI      NYK   0.581     111.6      41.8      86.9       48.1   \n",
       "\n",
       "   H_3PM_PG  H_3PA_PG  H_CUM_3P%  ...  A_DREB_PG  A_REB_PG  A_AST_PG  \\\n",
       "0      12.8      36.1       35.4  ...       33.9      44.9      27.5   \n",
       "1      12.0      36.4       33.0  ...       34.1      44.4      25.2   \n",
       "2      12.6      34.0       37.0  ...       33.3      44.5      25.0   \n",
       "3      12.9      36.7       35.2  ...       31.9      45.3      22.1   \n",
       "4      10.8      29.2       37.1  ...       34.7      46.3      21.6   \n",
       "\n",
       "   A_TOV_PG  A_STL_PG  A_BLK_PG  H_BLKA_PG2  A_PF_PG  A_PFD_PG  H_+/-_PG23  \n",
       "0      13.1       8.6       4.9         4.7     19.9      19.8         0.4  \n",
       "1      14.6       7.1       4.2         4.6     17.2      20.1         2.5  \n",
       "2      14.5       6.9       5.5         5.0     20.2      19.3        -3.1  \n",
       "3      12.5       8.9       4.6         5.1     19.7      19.2         2.0  \n",
       "4      13.4       7.0       4.9         4.7     20.5      20.4        -0.5  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bet = pd.read_csv(\"C:\\\\NBA_Betting\\\\328Games.csv\")\n",
    "Bet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make it Match\n",
    "\n",
    "To make predictions, the data frame needs to match the format (number of columns and column titles) of the training data.  To accomplish this, I need to remove the first two columns with the team codes.  I also need to remove and null values that could have snuck into the CSV unintentionally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bet = Bet.iloc[: , 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bet = Bet.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crystal Ball\n",
    "\n",
    "Now we call on the model to work its magic and make the predictions for tonight's games. We use the predict function and then create a list with the predictions for each game. They will be in same order as the input data, so make sure you don't forget which game is which just because we dropped the columns with team codes!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "GB_predict_Tonight=GB.predict(Bet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([230.43999468, 211.76630551, 230.43621412, 220.42287937,\n",
       "       238.98007558, 223.13676021, 237.57257982, 229.23819486,\n",
       "       215.54706441])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GB_predict_Tonight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make it Pretty\n",
    "\n",
    "Now for ease of use I like to create a new data frame from the resulting array and display it without indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tonights_Totals= pd.DataFrame(GB_predict_Tonight,columns=['scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_202e4\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_202e4_level0_col0\" class=\"col_heading level0 col0\" >scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_202e4_row0_col0\" class=\"data row0 col0\" >230.439995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_202e4_row1_col0\" class=\"data row1 col0\" >211.766306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_202e4_row2_col0\" class=\"data row2 col0\" >230.436214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_202e4_row3_col0\" class=\"data row3 col0\" >220.422879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_202e4_row4_col0\" class=\"data row4 col0\" >238.980076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_202e4_row5_col0\" class=\"data row5 col0\" >223.136760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_202e4_row6_col0\" class=\"data row6 col0\" >237.572580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_202e4_row7_col0\" class=\"data row7 col0\" >229.238195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_202e4_row8_col0\" class=\"data row8 col0\" >215.547064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x23f3e9301f0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tonights_Totals.style.hide(axis='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make your Money $$\n",
    "\n",
    "Now compare the predicted score to the Over/Under line offered by your local sportsbook!  When the predicted value is greater than the line, I bet the over.  When the opposite is true, I bet the under. Disclaimer: Bet legally, responsibly, and at your own risk. This model and betting strategy can become outdated and ineffective quickly and for a variety of reasons.  In the meantime, this model has been effective for me and my bets have hit about 58 percent of the time which exceeds 53 percent therefore covering the \"vig\" or \"juice\". My sample size is also smaller than a whole NBA season, but large enough to have me convinced that it really works. And not because my model is better at predicting final scores than Vegas, in fact the opposite is true. But I'll elaborate on the betting theory of why I believe this system works outside of this notebook.  \n",
    "\n",
    "I would view this as a fun experiment and not a money making endeavor.  Even if you could truly expect 5 percent returns on your money from betting Over/Unders in the NBA, the ROI is less than what you could expect to make investing in the market with a diversified portfolio. You can make about 7-9 percent on your money that way and it's a whole lot easier (source: Trust me bro).\n",
    "\n",
    "Lastly, the sportsbooks are under no obligation to welcome you as a customer.  If you start taking too much of their money they can cut you off and that's just no fun.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
